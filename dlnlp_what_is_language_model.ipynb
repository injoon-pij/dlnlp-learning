{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dlnlp_what_is_language_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPerSsx4Ai2TbWftTvkUhvM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/injoon-pij/dlnlp-learning/blob/main/dlnlp_what_is_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0COSUZ4QBX1"
      },
      "source": [
        "# 1) What is Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMKg2Wz5Q-JA"
      },
      "source": [
        "__언어 모델(Languagel Model)__ : 단어 시퀀스(문장)에 확률을 할당하는 모델\n",
        " * 언어 모델은 시퀀스들에 할당된 확률을 통해 보다 적절한 문장을 판단함\n",
        " * 즉, 가장 자연스러운 단어 시퀀스를 찾아내는 모델\n",
        " * 언어 모델에 사용하는 코퍼스가 언어 모델의 적용 분야(의료, 마케팅, 법...)와 얼마나 관련있는 코퍼스 데이터이냐에 따라 모델의 성능이 달라짐\n",
        "\n",
        "__언어 모델링(Language Modeling)__ : 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5D8LoSiS9N4"
      },
      "source": [
        "# 2) Statistical Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpS3eQLyTaqs"
      },
      "source": [
        "통계적인 접근 방법으로 언어 모델링을 수행하는 언어 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CYZdkS-T-ps"
      },
      "source": [
        "* 조건부 확률의 연쇄 법칙\n",
        " * $P(x_1, x_2, x_3 ... x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1 ... x_{n-1})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqWEC1jzUgXt"
      },
      "source": [
        "각 단어는 문맥이라는 관계로 인해 이전 단어의 영향을 받아 나온 단어이고 모든 단어로부터 하나의 문장이 완성되므로, 조건부 확률의 연쇄법칙을 문장의 확률 관점에서 적용하면 __문장의 확률은 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률들의 곱__으로 구성됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnDIkKYNUt8u"
      },
      "source": [
        "* $w_{1}$ , $w_{2}$ , $w_{3}$ ... $w_{n}$ 순서의 단어로 구성된 문장의 확률\n",
        " \n",
        " * $P(w_1, w_2, w_3, w_4, w_5, ... w_n) = \\prod_{n=1}^{n}P(w_{n} | w_{1}, ... , w_{n-1})$\n",
        "\n",
        " * (ex) $P(\\text{An adorable little boy is spreading smiles}) = P(\\text{An}) \\text{×}  P(\\text{adorable|An})  \\text{×}  P(\\text{little|An adorable})  \\text{×}  P(\\text{boy|An adorable little}) \\text{×}  P(\\text{is|An adorable little boy}) \\text{×}  P(\\text{spreading|An adorable little boy is})  \\text{×}  P(\\text{smiles|An adorable little boy is spreading})$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL0z2JMDWW8F"
      },
      "source": [
        "이전 단어로부터 다음 단어에 대한 확률은 코퍼스 데이터에서 각 문자 시퀀스가 등장한 횟수에 기반하여 계산함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv30y01AW6p0"
      },
      "source": [
        "* (ex) $P\\text{(is|An adorable little boy}) = \\frac{\\text{count(An adorable little boy is})}{\\text{count(An adorable little boy })}$\n",
        " * 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라고 하면, 위 단어 시퀀스의 등장확률은 30%가 됨\n",
        "\n",
        "* 단, 이러한 카운트 기반 접근 방식은 희소 문제(sparsity problem)이라는 한계가 존재함\n",
        " * 코퍼스 데이터의 크기가 아무리 커도 확률을 계산하고자 하는 단어 시퀀스가 코퍼스 내부에 없다면, 실생활에서 빈번히 사용되는 시퀀스라고 해도 단어 시퀀스에 대한 확률이 0이 되거나 아예 정의가 되지 못함. 이는 정확한 모델링 방법이 아님.\n",
        " * 즉, 충분한 데이터를 관측하지 못하여 언어를 정확하게 모델링하지 못하는 문제를 말함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVlC8QKTZGwg"
      },
      "source": [
        "# 3) N-gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9SPAwLFZJ_W"
      },
      "source": [
        "카운트에 기반한 통계적 접근을 사용하고 있으므로 통계적 언어 모델의 일종이나, 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어(n개)만 고려하는 접근 방법을 사용하는 언어 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc2MWVXtaDCN"
      },
      "source": [
        "기존의 통계적 언어 모델이 가지는 한계인 희소 문제의 원인은 코퍼스 데이터 내부에 확률을 계산하고자 하는 시퀀스나 단어가 존재하지 않을 수 있다는 점임. 그리고 이는 시퀀스의 길이가 길어질수록 존재하지 않을 확률이 높음.\n",
        "\n",
        "* N-gram 모델은 이러한 문제를 시퀀스 등장 확률 계산 시 __참고하는 이전 등장 단어들의 개수를 줄여 코퍼스에서 시퀀스를 카운트하지 못하는 경우를 감소__하는 것으로 해결하고자 함\n",
        "\n",
        "* (ex) $P(\\text{is|An adorable little boy}) \\approx\\ P(\\text{is|little boy})$\n",
        "\n",
        " * 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사한 값을 구함. 이렇게 하면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아짐."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tx_FqTCc2tW"
      },
      "source": [
        "N-gram 모델은 시퀀스를 구성하는 단어들을 n개의 단어 뭉치로 끊어서 하나의 토큰으로 간주\n",
        "\n",
        "(ex) An adorable little boy is spreading smiles\n",
        "\n",
        "* __uni__grams : an, adorable, little, boy, is, spreading, smiles\n",
        "* __bi__grams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles\n",
        "* __tri__grams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles\n",
        "* __4-__grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles\n",
        "\n",
        "N-gram 모델로 다음에 나올 단어를 예측할 때에는 이전에 등장한 n-1개의 단어에 의존하여 계산함.\n",
        "\n",
        "* 4-gram model : $P(w\\text{|boy is spreading}) = \\frac{\\text{count(boy is spreading}\\ w)}{\\text{count(boy is spreading)}}$\n",
        "* 예를들어 코퍼스에서 boy is spreading가 1,000번 등장하고, boy is spreading insults가 500번 등장했으면 boy is spreading 다음에 insults가 등장할 확률은 50%가 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liiVaB_uf6MZ"
      },
      "source": [
        "[한계]\n",
        "\n",
        "* 문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있지만, 여전히 n-gram에 대한 희소 문제가 존재\n",
        "\n",
        "* n-gram의 크기가 커지면 희소문제가 심각해지고, 반대로 작아지면 카운트 문제 발생은 줄어들겠지만 전체 문장을 고려한 언어 모델보다 훨씬 적은 단어들만을 보게 되므로 근사된 값의 정확도는 현실의 확률분포와 멀어짐\n",
        "\n",
        " * 예를 들어 '소년'을 수식하는 '작고 사랑스러운'이라는 어구를 반영하느냐 마느냐에 따라 뒤에 예측되는 단어가 달라질 수 있음\n",
        "\n",
        " * 권장되는 적절한 n의 사이즈는 최대 5라고 알려짐\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSoKi1VKipAo"
      },
      "source": [
        "# 4) Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2A4z3zXisf9"
      },
      "source": [
        "모델 내에서 모델의 성능을 수치화하여 결과를 내는 언어 모델에 대한 내부 평가 방식\n",
        " * 외부 평가 : 각 모델에 실제 작업을 시켜 정확도를 비교하는 작업. 비교 모델 수가 늘어날수록 시간이 매우 많이 필요한 단점 존재."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuRDioh_eHmV"
      },
      "source": [
        "__Pelplexity(PPL)__\n",
        "\n",
        "* \"헷갈리는 정도\"를 의미\n",
        "\n",
        "* 모델 내에서 모델의 성능을 수치화하여 결과를 내는 언어 모델에 대한 내부 평가 방식\n",
        " * 외부 평가 : 각 모델에 실제 작업을 시켜 정확도를 비교하는 작업. 비교 모델 수가 늘어날수록 시간이 매우 많이 필요한 단점 존재.\n",
        "\n",
        "* 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수(branching factor). 즉, 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하고 있는지를 의미.\n",
        "\n",
        " * 모델의 PPL이 10이 나온 경우, 해당 언어 모델은 테스트 데이터에 대해 다음 단어를 예측하는 모든 시점마다 평균적으로 10개의 단어 선택지를 고민하고 있음을 의미\n",
        "\n",
        "* 수치가 낮을수록 모델의 성능이 좋음을 의미\n",
        "\n",
        "* 단어의 개수($N$)로 정규화된 테스트 데이터에 대한 확률($P(w_{1}, w_{2}, w_{3}, ... , w_{N})$)의 역수\n",
        "\n",
        " * $PPL(W)=\\sqrt[N]{\\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}=\\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_{i}| w_{1}, w_{2}, ... , w_{i-1})}}$\n",
        "\n",
        " * (bigram) $PPL(W)=\\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_{i}| w_{i-1})}}$\n",
        "\n",
        "* 단, PPL 값은 테스트 데이터 상에서 모델의 성능을 의미하는 것이지, 사람이 느끼기에 좋은 언어 모델을 반드시 의미하지는 않음\n",
        "\n",
        "* 두 언어모델을 PPL값으로 비교할 때에는 크기가 크고, 도메인에 알맞은 테스트 데이터를 사용해야 신뢰도가 높아짐"
      ]
    }
  ]
}